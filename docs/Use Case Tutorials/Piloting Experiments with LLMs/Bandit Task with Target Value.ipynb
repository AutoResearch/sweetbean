{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Bandit Task with Target Value\n",
    "\n",
    "Bandit tasks are used to study human reinforcement learning behavior.  In this example, we demonstrate how to use SweetBean in combination with LLMs to determine experimental sequences that exceed random chance for human participants. In other words, we demonstrate how to use natural language experiments with synthetic participants to inform the design of web-based experiments with humans.  "
   ],
   "metadata": {
    "id": "Dr57hep3UjhD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Timeline\n",
    "\n",
    "Our goal is to counterbalance the reward values of the two bandits. Each bandit can either yield a reward or no reward under the following conditions:\n",
    "\n",
    "- If Bandit 1 yields a reward, Bandit 2 does not, and vice versa.\n",
    "- Each bandit yields a reward in 50% of the trials, ensuring balance.\n",
    "\n",
    "We design a total of 50 trials. Theoretically, a participant could achieve a maximum score of 50 points if they perfectly predict the bandits. However, with random choices, the expected score is 25 points.\n",
    "\n",
    "For this experiment, we aim to generate trial sequences where a simulated participant achieves at least 70% of the points. This allows us to investigate performance under conditions that exceed random chance but are not perfect.\n",
    "\n",
    "We begin by implementing a function that generates random reward sequences for the two bandits:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "sfcKaXexVdZm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "def get_random_timeline(n=50):\n",
    "  rewards = [0] * (n // 2) + [1] * (n // 2)\n",
    "  random.shuffle(rewards)\n",
    "  timeline = [{'bandit_1': {'color': 'orange', 'value': r}, 'bandit_2': {'color': 'blue', 'value': 1-r}} for r in rewards]\n",
    "  return timeline\n",
    "\n",
    "print(get_random_timeline(10))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oq6murOPVUrp",
    "outputId": "a1c55832-494e-42df-8f2e-dbafe8d2198a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment\n",
    "\n",
    "We create a function that returns a SweetBean two-armed bandit experiment\n",
    "\n",
    "Install SweetBean:"
   ],
   "metadata": {
    "id": "7-5aQy2YY_zQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!pip install sweetbean"
   ],
   "metadata": {
    "id": "5yH6D4uqZJan"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the function"
   ],
   "metadata": {
    "id": "T9aZpS21ZXZ-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from os import truncate\n",
    "from sweetbean import Experiment, Block\n",
    "from sweetbean.variable import (\n",
    "  TimelineVariable, SharedVariable, DataVariable,\n",
    "  FunctionVariable, SideEffect\n",
    ")\n",
    "from sweetbean.stimulus import Bandit, Text\n",
    "\n",
    "\n",
    "def get_experiment(timeline):\n",
    "  bandit_1 = TimelineVariable(\"bandit_1\")\n",
    "  bandit_2 = TimelineVariable(\"bandit_2\")\n",
    "\n",
    "  score = SharedVariable(\"score\", 0)\n",
    "  value = DataVariable(\"value\", 0)\n",
    "\n",
    "  # here, we set an identifier to make it easier to filter the correct\n",
    "  # trials from the data\n",
    "  bandit_identifier = DataVariable(\"is_bandit_task\", 0)\n",
    "\n",
    "  update_score = FunctionVariable(\n",
    "    \"update_score\", lambda sc, val: sc + val, [score, value]\n",
    "  )\n",
    "\n",
    "\n",
    "  update_score_side_effect = SideEffect(score, update_score)\n",
    "  add_identifier = SideEffect(bandit_identifier, truncate)\n",
    "\n",
    "  bandit_task = Bandit(\n",
    "    bandits=[bandit_1, bandit_2],\n",
    "    side_effects=[update_score_side_effect, add_identifier],\n",
    "  )\n",
    "  show_score = Text(duration=1000, text=score)\n",
    "  block = Block([bandit_task, show_score], timeline=timeline)\n",
    "  experiment = Experiment([block])\n",
    "  return experiment"
   ],
   "metadata": {
    "id": "F1VTPCv3ZZvw"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's test the experiment as html file:"
   ],
   "metadata": {
    "id": "pQD5_-x8bhxV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "timeline = get_random_timeline(10)\n",
    "experiment = get_experiment(timeline)\n",
    "experiment.to_html('bandit.html')"
   ],
   "metadata": {
    "id": "A6trnT6Vbe1r"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LLM participant\n",
    "\n",
    "After confirming that the html file is as expected by running it, we can create a synthetic participant by using the centaur model\n",
    "\n",
    "Installing the dependencies:"
   ],
   "metadata": {
    "id": "esr0I7rPqPXZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!pip install unsloth \"xformers==0.0.28.post2\""
   ],
   "metadata": {
    "id": "hRoo9MAoqLb6"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating a generate function:"
   ],
   "metadata": {
    "id": "ZcZhaUr0qwwM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import transformers\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"marcelbinz/Llama-3.1-Centaur-8B-adapter\",\n",
    "    max_seq_length=32768,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "pipe = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    trust_remote_code=True,\n",
    "    pad_token_id=0,\n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    "    max_new_tokens=1,\n",
    ")\n",
    "\n",
    "\n",
    "def generate(prompt):\n",
    "    return pipe(prompt)[0][\"generated_text\"][len(prompt):]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mBNQ0e_mqzd-",
    "outputId": "84b25153-45d4-4821-db8e-d76610a040cd"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "First, let's simulate a single experiment:",
   "metadata": {
    "id": "E2JTfeeEVV5V"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "ib8masuZuPEB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "timeline = get_random_timeline(10)\n",
    "experiment = get_experiment(timeline)\n",
    "data, _ = experiment.run_on_language(get_input=generate)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "tdpev5_VsMYY",
    "outputId": "51fcef8e-1bf7-481a-efc2-1333c962d69a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "... and look at the data:",
   "metadata": {
    "id": "lJp_kb5Jeuzd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(data)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B7HEVck3eL9X",
    "outputId": "7249840b-4194-4b8a-aa6a-82a6dec7efa2"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "We can filter out the \"is_bandit_task\" trials and get the chosen values",
   "metadata": {
    "id": "k-_dass3exa2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data_values = [d['value'] for d in data if 'is_bandit_task' in d and d['is_bandit_task']]\n",
    "print(data_values)\n",
    "print(sum(data_values)/len(timeline))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qdpHJMYSe8hU",
    "outputId": "be8e95af-4b5c-4f05-c2fc-e004ada5550a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's define a function for the full simulation"
   ],
   "metadata": {
    "id": "jEeBW0RijxDf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def simulation(n):\n",
    "  timeline = get_random_timeline(n)\n",
    "  experiment = get_experiment(timeline)\n",
    "  data, _ = experiment.run_on_language(get_input=generate)\n",
    "  data_values = [d['value'] for d in data if 'is_bandit_task' in d and d['is_bandit_task']]\n",
    "  return sum(data_values)/n, timeline"
   ],
   "metadata": {
    "id": "cT0BJylUj6Th"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Now, we can create a loop that simulates until a threshold of 70% is reached and stores the timeline of the reward sequence. To speed up things here, we only simulate 20 trials. (In a real application instead of creating random sequences, one would vary the sequences more systematically. For example, by applying drifts to the reward probabilities)",
   "metadata": {
    "id": "EmhYWqyEkKRv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "value_percentage = 0\n",
    "while value_percentage < 0.7:\n",
    "  value_percentage, timeline = simulation(20)\n",
    "  print()\n",
    "  print(value_percentage)\n",
    "\n",
    "print(timeline)\n",
    "\n",
    "with open('timeline.json', 'w') as f:\n",
    "  json.dump(timeline, f)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yE_JVbt5kXjt",
    "outputId": "9e011a56-9197-4408-95e7-9674d66b1faa"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Let's rerun the experiment on the same timeline to check if the llm just got lucky or if a similar average value can be achieved:",
   "metadata": {
    "id": "sJS-3J9pnydc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "experiment = get_experiment(timeline)\n",
    "data, _ = experiment.run_on_language(get_input=generate)\n",
    "data_values = [d['value'] for d in data if 'is_bandit_task' in d and d['is_bandit_task']]\n",
    "print(sum(data_values)/len(timeline))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tlnCZ8gvnwuu",
    "outputId": "d52f1682-b439-408e-abb9-81388583cc6e"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "\n",
    "SweetBean can be used to pilot experiments. Afterward, one could *manually* set up the experiment with the same timeline and run it on human participants or use [*AutoRA*](https://autoresearch.github.io/autora/) to comfortably automate the process of hosting the same experiment and collecting the data online and even run experiments in a closed loop to iteratively improve the experiment with a mixture of simulated and human data."
   ],
   "metadata": {
    "id": "tCIwlaezoQlp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "pWUS5dUIjh_C"
   }
  }
 ]
}
